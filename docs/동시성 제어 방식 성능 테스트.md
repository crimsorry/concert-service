### 🧩 동시성 발생 비즈니스 로직

1. 잔액 충전
2. 콘서트 예약
3. 콘서트 결제



### 🧩 정책 설정

1. 잔액 충전의 경우 **PG** 를 통해 결제가 이루어졌다 가정하고 충전이 발생하면 **모든 요청**은 포인트로 충전되어야 한다.
2. 한 좌석 당 **한 명의 사용자**만 점유할 수 있다.



### 🧩 동시성 제어 방식

#### 1. DB - 비관적 락 (Pessimistic Lock)

- 데이터에 대한 경합이 빈번하게 발생할 것으로 예상하고, 데이터에 대한 액세스를 사전에 차단하여 충돌을 방지하는 기법
- **실패한 요청에 대해 db threadpool 저장해두고 재사용.**
- 읽기 막음

**장점**: 일관성 보장, 데이터 충돌 사전 방지

**단점**:  다른 트랜잭션의 대기 시간 길어짐, **데드락** 발생할 수 있음



#### 2. DB - 낙관적 락 (Optimistic Lock)

- 데이터 갱신 시 경합이 자주 발생하지 않을 것이라 가정하고 잠금을 거는 기법
- **1개 성공하면 나머지 요청 실패 처리** 혹은 **retry**
- 읽기 막지 않음

**장점**: 데이터 충돌이 드문 경우 효율적

**단점**:  대용량 트래픽의 경우 경합 문제가 발생, 구현의 복잡성



#### 2-1 DB 사용 단점

* **DB 부하 증가**: 경합 증가 시 트랜잭션 대기 시간이 길어져 전체 응답 성능에 영향을 미칠 수 있다
* **확장성 한계**: 높은 동시성 요청 처리에 있어 DB 성능이 병목이 될 수 있으며, 수평적 확장이 어려움.



#### 분산락 사용 이유

* 부하분산을 위해, 분산환경에서 사용.



#### 3. Redis - Simple Lock

- Redis의 **단일 키**를 사용하여 간단히 락을 걸고 해제하는 기법.

**장점**: 단순한 동시성 제어에 적합, 구현 간단, 빠른 속도

**단점**: 재시도 로직 고려 필요, 실패 빈도 높음



#### 4. Redis - Spin Lock

- 일정 시간 동안 Redis에서 락 획득을 시도하는 기법 > **while 문**

**장점**: 반드시 하나의 클라이언트만 락 획득, 작업 후 락 해제.

**단점**: 재시도 실패시 무한루프, Redis 부하 증가



#### 5. Redis - Pub Sub

- Redis의 Pub/Sub 모델을 이용하여 여러 클라이언트가 **이벤트 기반**으로 락을 제어하는 기법
- 락 해제 시 클라이언트에게 다시 락 획득 신호 전달(TTL) > **데드락** 방지

**장점**: 여러 인스턴스에 접근 가능

**단점**: 구현 복잡, 비용 증가



#### 6. Kafka

- Kafka의 메시지 큐 기능을 사용해 순차적 메시지 처리와 분산 시스템에서의 동기성 제어하는 기법
- **락은 아님**

**장점**: 높은 확장성과 성능을 보장하며, 복잡한 트랜잭션을 관리하는 데 적합.

**단점**: 추가적인 인프라 세팅 필요



#### ETC. Redis  VS Kafka 

* **Redis**
  * **휘발성 데이터** > bgsave 는 snapshot 
  * **Broad cast**: publish 후 모든 클라이언트에게 메시지 전송
  * 동시성 제어, 데이터베이스 쿼리, 이벤트 처리 등 다양하게 사용 가능
* **Kafka**
  * **비휘발성 데이터**: 발생 이벤트 partition  저장
  * **중앙 저장 방식**: 중앙에 데이터 저장 후 필요한 consume 에서 데이터 가져감



|                         | 구현 복잡도 | 성능                       | 효율성             |
| ----------------------- | ----------- | -------------------------- | ------------------ |
| **비관락**              | 낮음        | 낮음                       | 낮음               |
| **낙관락**              | 보통        | 경합에 따라 달라짐         | 경합에 따라 달라짐 |
| **Redis - Simple Lock** | 보통        | 보통                       | 보통               |
| **Redis - Spin Lock**   | 보통        | 보통                       | 낮음. 부하 증가    |
| **Redis - Pub Sub**     | 보통        | 보통 (Redis, DB 접근 필요) | 보통               |
| **Kafka**               | 높음        | 높음                       | 높음               |



### 🧩 동시성 선택

<details>
<summary>✨ <b>선정 과정</b> (< Click)</summary>
<h4>1. 잔액 충전</h4>
<p><b>고려방안</b>: 동일 유저가 잔액을 충전함으로 많은 충돌이 발생하지 않기 때문에 낙관적 락을 사용해야 한다. 충돌이 발생하는 경우도 사용자가 창을 여러 개 띄우는 경우 혹은 비정상적인 접근이기 때문이다. 낙관적 락으로 구현 가능한 경우의 수를 조사하였다.</p>
<ul>
  <li><b>낙관락 - 경합 시 throw Exception</b>
    <blockquote>
      <p>충돌이 났다는 건 포인트에 사용자가 동시에 접근했다는 의미. (비정상적인 접근) 그렇다면 최초 요청 한 번만 처리하고 나머지는 exception이 맞다.</p>
      <p>요청으로 1원이 5번 들어온 경우, 최초의 요청인 1원만 최종적으로 충전되어야 한다.</p>
    </blockquote>
  </li>
  <li><b>낙관락 - 경합 시 retry</b>
    <blockquote>
      <p>충돌이 일어나도 다시 retry를 해서 충전이 모두 들어가야 한다. 설정된 정책 상으로 사용자의 포인트 충전 요청은 PG로부터 결제 완료 후 들어오게 된 요청이다.</p>
      <p>요청으로 1원이 5번 들어온 경우, 5원이 최종적으로 충전되어야 한다.</p>
    </blockquote>
  </li>
</ul>
<p>정책 설정에서 충전 요청은 <b>PG로부터 결제 완료 후 들어온 요청</b>이라는 항목이 있으므로 낙관적 락으로 구현하게 된다면 retry가 필요할 것이라 생각했다. 그렇게 생각하니 생각나는 또 다른 경우는...</p>
<ul>
  <li><b>비관락 ✅</b>
    <blockquote>
      <p>모든 요청은 DB에 들어가야 하기 때문에 비관락을 고려하게 되었다. 비록 낙관락보다 성능상으로 낮아지는 부분도 존재하지만, 낙관락을 이용한 retry 방식보다 비관락을 사용하는 것이 <b>누락이 없는</b> 안전한 방향이라 생각해 선정하게 되었다.</p>
    </blockquote>
  </li>
  <li>분산락은?
    <blockquote>
      <p>최종적인 방향은 비관락을 분산락으로 이용하는 방법이 아닐까, 라는 생각이 들지만 지금은 우선 DB를 이용한 락 구현에 집중하고자 한다.</p>
    </blockquote>
  </li>
</ul>
<br>
<h4>2. 콘서트 예약</h4>
<p><b>고려방안</b>: 동일한 좌석에 대해 다량의 요청이 동시에 발생하기 때문에 낙관적 락으로 생각했다.</p>
<p>그런데..</p>
<ul>
  <li><b>낙관락 - 경합 시 throw Exception</b>
    <blockquote>
      <p>첫 번째 요청을 제외한 요청들을 catch 시키면 부하가 줄어들 것이라 생각했다. 하지만 첫 번째 요청에서 (만약) 데이터베이스 등과 같은 에러가 생겨 1000번 이상의 요청자, 즉 경합을 하지 않았던 사용자가 좌석을 선택하게 되는 경우가 발생할 수 있으므로 비관락이 낫지 않을까? 하는 코치님의 의견에 설득당해 <b>비관락</b>을 최종적으로 고려하게 되었다.</p>
    </blockquote>
  </li>
  <li><b>비관락 ✅</b></li>
</ul>
<br>
<h4>3. 콘서트 결제</h4>
<p><b>고려방안</b>: 동일 유저가 예약한 콘서트를 결제하기 때문에 많은 충돌이 일어나지 않을 것이라 예상했다. 동시 요청이 들어와도 하나의 요청을 제외한 나머지 요청은 실패처리가 되어야 하기 때문에 낙관락을 선택하게 되었다.</p>
<ul>
  <li><b>낙관락 - 경합 시 throw Exception ✅</b>
      <blockquote>
      <p>결제 역시 PG를 거치게 되지만 모든 요청이 성공해야 하는 충전과는 달리 하나의 요청만 성공해야 하기때문에 낙관락을 선택했다.</p>
    </blockquote>
  </li>
</ul>
</details>


#### 1. 잔액 충전

```
선정: 비관락

선정사유: 동일 유저의 모든 요청은 PG로부터 결제 완료 후 들어온 요청이 들어오기 때문에 
모든 요청에 대해 충전이 필요하다. 따라서 안정성을 위해 선정 
(자세한 사유 선정과정 참조)

정확도: 높음

동시성 발생: 낮음
```

#### 2. 콘서트 예약

```
선정: 비관락

선정사유: 동일한 좌석에 대해 다량의 요청이 동시에 발생하기 때문에 
A요청 이후의 요청들은 thread pool 상태여야 한다고 생각해 선정 
(자세한 사유 선정과정 참조)

정확도: 보통 (대기열이 한번에 들어오는 경우의 순서는 보장 X)

동시성 발생: 높음
```

#### 3. 콘서트 결제

```
선정: 낙관락

선정사유: 동일 유저가 예약된 콘서트를 결제하기 때문에 
경합 발생 가능성이 낮다고 생각해 선정 
(자세한 사유 선정과정 참조)

재시도: 무

정확도: 높음

동시성 발생: 낮음
```



### 🧩 성능 테스트

#### 1. 잔액 충전

* **비관락 ✅![잔액충전_비관락_3](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_비관락_3.png)![잔액충전_비관락_10](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_비관락_10.png)**

  ```
  성능: 확실히 비관적 락이 낙관적 락에 비해 성능은 느리지만 
  모든 요청이 안정적으로 반영되는 것으로 확인되었다.
  
  다음으로 낙관적락으로 잔액 충전을 테스트하면 어떤 결과값이 나오는지 
  테스트해보기로 하였다.
  ```

* **낙관락 - 경합 시 throw Exception > Non-Repeatable Read![잔액충전_낙관락_3](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_낙관락_3.png)**

  ```
  성능: 비관적 락에 비해 빠른 성능을 보였다.
  
  문제상황: 대량의 데이터가 들어오는 상황에서 경합으로 인해 
  1원의 충전이 반영되지 않는 경우를 테스트했다. 
  예상했던 결과는 99건이 실패하는 것이었으나, 실제로는 32건이 실패했다. 
  로그를 확인한 결과는 다음과 같았다.
  
  A 요청 > 트랜잭션 > version 확인 (OK!) > update version +1 > commit
  B 요청 > 트랜잭션 > version 확인 (error! A요청 점유 중) >
  ...
  F 요청 > 트랜잭션 > version 확인 (OK! A요청 commit 완료) > update version +1 >
  
  위와 같이 Non-Repeatable Read, 경합 문제가 발생하였다. 
  
  그러나 이 상황은 문제로 볼 수 없다. 이는 동일한 버전에 대한 
  동시 요청이 발생할 때 경합 조건에 따라 일부 요청이 throw로 처리되고 있기 때문이다.
  
  원인은 콘서트 예약 시스템과 비교했기 때문. 
  콘서트 예약 시스템에서는 다수의 요청이 동시에 들어와도 
  하나의 좌석은 단 한 명만 점유할 수 있도록 동시성 제어가 이루어진다. 
  반면, 충전 요청의 경우 중복 요청에 대한 처리가 따로 되어 있지 않아 
  충돌이 발생한 것이다.
  
  해결 방안: 중복 충전 문제를 방지하기 위해 트랜잭션 ID와 같은 
  고유값을 생성하여 중복된 충전 요청을 제한하는 방식이 필요하다. 
  현재 비관적 락을 사용하기로 결정했으며, 
  유사한 상황에서 발생할 수 있는 동시성 이슈에 대비해 
  낙관적 락의 throw 방식 또한 검토할 계획이다.
  
  이와 같이 충전과 같은 동시성 요구가 높은 상황에서는 
  고유 식별자를 활용한 중복 방지와 적절한 락 선택이 중요하다.
  ```
  
* **낙관락 - 경합 시 retry![잔액충전_낙관락_retry_3](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_낙관락_retry_3.png)**

1. test 100건: service: @Transaction, @Retry 모두 사용

   ![잔액충전_낙관락_retry_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_낙관락_retry_100.png)

2. test 1000건: 계층 분리. service: @Retry / port: @Transactional![잔액충전_낙관락_retry_1000_service_분리](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_낙관락_retry_1000_service_분리.png)

3. test 1000건: service: @Transaction, @Retry 모두 사용
   ![잔액충전_낙관락_retry_1000](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_낙관락_retry_1000.png)

   ```
   성능: 비관락보다 성능은 낮았다. 
   그러나 retry 간격 조절에 따라 다른 값이 나올 것이라 예상한다.
   
   문제상황: @Tranaction과 @Retry 를 같이 사용 시 충돌이 발생할 수 있다는 내용 참고
   
   @Transactional: 트랜잭션 단위로 동작하며, 실패하는 경우 롤백을 수행 해 데이터 일관성 보장
   @Retry: 낙관적 락에서 version 이 같다면 catch 에서 예외를 발생해, 실패 시 재시도를 통해 일관성 유지
   
   두 애너테이션이 유사한 역할을 하므로, 동일한 메서드에서 사용할 경우 
   불필요한 충돌을 초래할 수 있다는 점에서 이들 기능을 분리할 필요가 있다는 의견.
   이러한 맥락에서 @Transactional과 @Retry를 분리하여 
   Facade 패턴을 통해 제어하는 설계가 필요할지 고려하게 되었다.
   
   혹시나 성능상의 문제가 있을까 싶어 아래와 같은 기준으로 테스트 코드로 성능 테스트를 진행해보았다.
   
   전재조건: testcase 1000건 
   1 case: 계층 분리. service: @Retry / port: @Transactional
   2 case: service: @Transaction, @Retry 모두 사용.
   
   결과:
   1 case: 6.783s
   2 case: 4.548s
   
   결과적으로 service 는 분리하지 않는 편이 더 빠른 성능을 보였고 같이 사용한 경우 오류는 발견하지 못했다. 
   그리고 자문을 구한 결과 @Retry 를 하게 되면 @Transaction 이 전파하기 때문에 
   오류가 발생한 경우는 거의 없다고 보는게 무방하다는 말을 수용하였다. (usecase 사용)
   ```
   

* **redis - pub/sub**

  ![잔액충전_레디스_3](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_레디스_3.png)![잔액충전_레디스_10](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_레디스_10.png)
  ![잔액충전_레디스_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/잔액충전_레디스_100.png)

  ```
  성능: 데이터를 저장하고 consume하는 Kafka랑 달리, 데이터를 push하고 
  대기 시간을 거쳐 가져오는 방식이라 대기 시간(delay) 설정에 따라 처리 시간이 달라질 수 있다. 
  redis 는 DB와 redis 모두 거치기 때문에 상대적으로 응답 시간이 더 오래 걸리게 된다.
  이런 특성 덕에 Redis는 마이크로서비스 아키텍처(MSA) 같은 분산 환경에서 동시성 제어나 캐싱 목적으로 많이 쓰인다.
  
  6주차에는 DB 락에 집중하고 7주차부터 redis 와 kafka 직접 테스트 후 비교해서 적용해보려고 한다.
  ```
  
  

#### 2. 콘서트 예약

* **비관락 **✅![예매_비관락_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_비관락_100.png)![예매_비관락_1000](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_비관락_1000.png)

  ```
  성능: 비관락은 예상대로 가장 낮은 성능을 보였으나, 
  대기열에 진입한 사용자에게 일관된 예약 상태를 보장하기 위해 선택되었다. 
  특히, 높은 경합 상황에서도 사용자 요청이 순서대로 처리되어야 하기 때문에, 
  신뢰성과 일관성을 최우선으로 고려하여 비관적 락을 채택했다.
  
  다음으로 낙관적락으로 잔액 충전을 테스트하면 어떤 결과값이 나오는지 테스트해보기로 하였다.
  ```

* **낙관락 - 경합 시 throw Exception**
  ![예매_낙관락_throw_100_2](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_낙관락_throw_100_2.png)
  ![예매_낙관락_throw_1000](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_낙관락_throw_1000.png)

  ```
  성능: 첫 번째 요청을 성공 처리하고 나머지 요청을 예외로 처리하는 낙관적 락 방식이 가장 우수한 성능을 보였다.
  
  그러나 1000명 수준의 요청이 들어온 경우, 재시도(retry) 방식과 성능 차이가 크지 않았으며, 
  이는 캐싱 전략에 따라 속도 차이가 발생했기 때문으로 보인다.
  ```
  
* **낙관락 - 경합 시 retry**

  ![예매_낙관락_retry_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_낙관락_retry_100.png)
  ![예매_낙관락_retry_1000](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_낙관락_retry_1000.png)

  ```
  성능: 첫 번째 요청이 가장 시간이 소요되었으나, 두 번째 요청부터는 처리 속도가 크게 향상되며 가장 빠른 성능을 보였다.
  ```

* **redis - pub/sub**

  ![예매_redis_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_redis_100.png)

  ```
  성능: 앞선 테스트와 유사하게, **대기 시간(delay)**으로 인해 성능이 가장 저하되는 결과를 보였다.
  ```

  

#### 3. 콘서트 결제

* **비관락**

  ![결제_비관락_5](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_비관락_5.png)![결제_비관락_10](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_비관락_10.png)
  ![결제_비관락_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_비관락_100.png)

  ```
  성능: 처리 속도는 중간 수준으로, 낙관적 락의 예외 처리 방식보다는 다소 느린 결과를 보였다.
  ```

* **낙관락 - 경합 시 throw Exception✅**

  ![결제_낙관락_throw_5](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_throw_5.png)
  ![결제_낙관락_throw_10](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_throw_10.png)
  ![결제_낙관락_throw_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_throw_100.png)

  ```
  성능: 낙관적 락의 예외 처리 방식(throw)이 가장 빠른 처리 속도를 보였다.
  
  문제상황: 결제 확인을 위한 amountHistory (결제 내역 테이블)에 값이 정확히 입력되었는지 검증하는 과정에서 오류가 발생했다.
  ```

  ```java
  [Test 코드]
  AmountHistory amountHistory = amountHistoryRepository.findByPointId(1L);
  assertEquals(testBase.payment.getAmount(), amountHistory.getAmount());
  ```

  ```
  그러나 3/5 빈도로 amountHistory가 null인 상황이 발생하며 테스트가 실패했다.
  
  원인 분석: 원인 파악 결과, 트랜잭션 롤백 시 auto-increment 값이 초기화될 것으로 예상했으나, 
  실제로는 초기화되지 않고 유지되고 있었다. 
  이를 해결하기 위해 하드코딩된 ID 참조 대신, 동적으로 ID를 관리하는 방식으로 수정하였다.
  
  throw > auto increment + 1 > transaction 롤백 
  
  결론: amountHistoryRepository.finaAll() 로 테스트는 검증한 결과 성공하게 되었다.
  ```

* **낙관락 - 경합 시 retry**

  ![결제_낙관락_retry_5](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_retry_5.png)![결제_낙관락_retry_10](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_retry_10.png)
  ![결제_낙관락_retry_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/결제_낙관락_retry_100.png)

  ```
  성능: 예상과 달리, 비관적 락보다 성능이 저하되었다. 이는 재시도(retry)로 인한 속도 지연이 원인으로 추정된다.
  ```

* **redis - pub/sub**

  ![예매_redis_100](https://github.com/crimsorry/hhplus-concert-service/tree/main/docs/picture/예매_redis_100.png)

  ```
  성능: 이전과 마찬가지로 지연(delay)으로 인해 성능이 가장 저하된 결과를 보였다.
  ```

  

#### 출처

https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html

https://helloworld.kurly.com/blog/distributed-redisson-lock/

https://devoong2.tistory.com/entry/Spring-Redisson-TryLock-%EB%8F%99%EC%9E%91-%EA%B3%BC%EC%A0%95