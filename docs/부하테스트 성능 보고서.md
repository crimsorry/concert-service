# 🧩 부하테스트 + 장애 대응 보고서



## 🧩 테스트 환경

- OS: Window
- CPU: 8 Core
- Memory: 16GB
- Storage: 220GB SSD
- 실행환경: Docker
- 부하테스트: K6
- 성능 확인 도구: InfluxDB, Grafana



## 🧩 개요

* **시스템 한계점 분석**
  - **부하가 집중될 가능성이 높은 시나리오**를 기반으로 시스템의 병목 현상을 식별.
  - 병목 구간을 파악한 후 **리팩토링 및 최적화 전략**을 통해 성능 개선.
* **Stress Test (스트레스 테스트)**
  - 시스템이 처리할 수 있는 **최대 사용자 요청 수**를 측정하여, 한계점 분석.
  - 최대 부하 조건에서 발생 가능한 문제를 확인하고, **스케일링 전략** 도출.
* **Load Test (부하 테스트)**
  * **안정적으로 처리 가능한 사용자 요청 수**를 측정
  * 효율성과 안정성을 모두 충족하는 **최적 운영 조건** 설정.



## 🧩 테스트 시나리오

| 케이스                            | 예상 시나리오           | 실제 결과                  |
| --------------------------------- | ----------------------- | -------------------------- |
| 대기열 생성                       | 동시 접속자 수 : 10,000 | 동시 접속자 수 : 12,000    |
| 대기열 생성 > 콘서트 조회         | 캐시 응답시간 일정      | 캐시 미스 > 원인분석       |
| 대기열 생성 &gt; 콘서트 일정 조회 | 동시 접속자 수 : 600    | connection poll > 원인분석 |



## 🧩 병목 발생

#### 1. Redis 부하

* 대량 데이터를 캐싱하며 메모리 사용량 급증 및 성능 저하 발생.

#### 2. 캐싱 전략 부재

* 비효율적인 캐싱 설계로 인해 캐시 적중률 감소 및 Redis 성능 저하.

#### 3. 데이터베이스 부하

* 과도한 데이터 조회 요청이 데이터베이스에 집중되면서 응답 속도 지연.

#### 4. 서버 리소스 부족

* 최소 리소스 설정(CPU 1 core, 메모리 500MB)으로 인해 시스템 메모리 고갈 및 처리 한계 초과.



## 🧩 시스템 한계

* **Local 환경 테스트**
  * 테스트는 **단일 노드 환경**에서 수행되며, **서버 분리**가 이루어지지 않아 트래픽 집중 시 **자원 경쟁(resource contention)**으로 인해 전반적인 성능 저하 및 처리 지연(latency)이 발생할 가능성이 높다.
  * 본 테스트는 **시스템의 처리 한계(capacity limit)**와 **최적 사용자 요청 처리량(optimal throughput)**을 식별하여, **확장성(scalability)** 및 **성능 튜닝(performance tuning)**을 위한 개선 방향을 도출하기 위한 목적으로 수행된다.



## 🧩 시스템 개선

#### 1. 캐싱 전략 최적화

* Redis 캐싱 데이터 범위를 축소하고, 자주 호출되는 데이터만 선별적으로 캐싱.
* 페이지 단위 캐싱을 적용해 Redis 및 네트워크 부하 분산.

#### 2. 리소스 증설

* **CPU**와 **메모리**를 요청 처리량에 맞게 증설(CPU 2 core, 메모리 1GB 이상).
* JVM 힙 메모리 설정 최적화(`-Xms768m -Xmx1g`)로 OutOfMemoryError 방지.

#### 3. connection poll 추가

* HikariCP를 사용해 데이터베이스 연결을 효율적으로 관리.



## 🧩 추가 개선 전략

#### 1. 서킷 브레이커 도입

* **슬라이딩 윈도우(sliding window)**를 활용해 임계치를 초과한 요청을 감지하고, 장애 발생 시 요청을 제한.
* 특정 노드의 과부하를 방지하고 전체 시스템 안정성 향상을 위해 적용

#### 2. 클러스터 스케일링 (Cluster Scaling)

* Redis Cluster 를 구상해 장애 발생 시에도 **고가용성(High Availability, HA)**을 유지하도록 클러스터링 적용

#### 3. High Associativity 구조 

* 데이터 파티셔닝(Sharding)을 통해 요청이 분산되도록 설정



## 🧩 선정 사유

#### 대기열 생성

* **선정 사유:**
  
  * SPOF > 대기열 생성이 제대로 되지 않으면 후 로직 처리 불가능
  
* **주요 테스트:**
  
  * 분당 최대 동시 사용자 수 파악 
    ```
    인기 콘서트의 경우 동시 접속자 수가 수 만명 이상 몰리게 된다. 따라서 이번 프로젝트에서는 분당 동시 접속자 수를 10,000 명 설정하고 테스트를 진행하면서 최대 동시 접속자 수를 파악하려 한다.
    ```
  
  * 분당 최적 동시 사용자 수 파악
  
    ```
    1초 이내 응답 가능한 최적 동시 접속자 수를 파악한다.
    ```
  
  * 고유 대기열 토큰 발급 확인
  
    ```
    return 이 제대로 표출되는지 확인한다.
    ```
  
  * 시스템 리소스 분석
  
    ```
    시스템 부하가 걸리는 정도를 파악하고자 한다.
    ```
  
    

#### 콘서트 목록 조회

* **선정 사유:**
  * 캐시 성능 확인
  * 일정한 응답시간 확인
  
* **주요 테스트:**
  * 캐시 히트율	
  
    ```
    캐시 미스를 최대한 줄이기 위해 노력한다.
    ```
  
  * 응답 시간 분포
  
    ```ㅂ
    동시접속자가 10,000 이상 들어온 경우 응답 시간이 동일하게 호출되는지 확인하고자 한다.
    ```
  
  

#### 대기열 생성 > 콘서트 일정 조회

* **선정 사유:**
  
  * 대기열 진입 후 최초 접속하게 되는 API. 10초당 접속자 수를 파악하기 위해 선정
  * 인덱스 조회 성능 분석
  
* **주요 테스트:**
  
  * 최대 동시 접속자 수 파악
  
    ```
    이용자는 폴링을 통해 대기열에 진입 후 일정을 조회 할 수 있다. 따라서 분당 최소 동시 접속자 수는 6,000명으로 선정한다. 테스트를 통해 최적 동시 접속자 수를 파악한다.
    ```
  
  * 최적 동시 접속자 수 파악
  
    ```
    대기열 부하를 줄일 수 있는 최적 사용자 수를 파악한다.
    ```



## 🧩 주요 테스트 확인 

* **http_req_connecting** : 응답 시간
* **P90** : 90% 사용자 평균 응답 시간
* **P95** : 95% 사용자 평균 응답 시간



## 🧩 대기열 생성 테스트 결과

#### 테스트 스크립트

```javascript
import http from 'k6/http';
import { check, sleep, fail } from 'k6';

export let options = {
    startVUs: 0, // 시작 VU 수
    stages: [
    { duration: '15s', target: 200 },
    { duration: '15s', target: 450 },
    { duration: '15s', target: 200 },
    { duration: '15s', target: 0 },
    ],
    thresholds: {
        http_req_duration: ['p(90)<1000', 'p(95)<1000'], // 90% : 1초 미만/ 99% : 1초 미만
        http_req_failed: ['rate<0.01'],                  // 실패율 : 1% 미만
        checks: ['rate>0.99'],                           // 전체 요청 성공률 : 99% 이상
    },
    ext: {
      influxdb: {
        enabled: true,
        address: 'http://localhost:8086', 
        database: 'k6'
      },
    }
};

const BASE_URL = 'http://localhost:8081/api/v1';
const userIdStart = 36;
const MAX_RETRIES = 5;

// 대기열 토큰 발급
function issueWaitingTokenWithRetry(userId) {
    let retries = 0;
    let response;
    let success = false;

    while (retries < MAX_RETRIES && !success) {
        response = http.post(`${BASE_URL}/user/${userId}/queue/token/issue`);
        success = check(response, {
            '토큰 발급 성공': (r) => r.status === 200,
        });

        if (!success) {
            retries++;
            sleep(1); 
        }
    }
    
    const responseBody = response && response.body ? response.json() : null;

    if (!success) {
        fail(`UserId: ${userId} 재시도 횟수: ${MAX_RETRIES}, 상태 코드 ${responseBodystatus}, 응답: ${responseBody.body}`);
    }

    return responseBody ? responseBody.waitingToken : null;
}

export default function () {
    let userId = userIdStart + __VU;    
    issueWaitingTokenWithRetry(userId);
}
```



### **최대 처리 용량 테스트**

`주요 관점 포인트: 응답 성공`

* 6,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjdfMTcg/MDAxNzMyNzE0Mjg1Mzky.tiR5OJBDYaDkXKujS8XfR4vO0Tzedjx59o1jxEdWw54g.TzvQY7iG2LDjYBBgqTiQ6rxQ5eaEKtVkNawu1Mx0GB4g.PNG/image.png?type=w773)

* 12,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjdfMTgw/MDAxNzMyNzE1MjM4MDQ2.jmU2CTv4IiJBbcp618xoJZB73goPOfr9bkAkGuhTU1Yg.JcDPGbTf9_0xnv9LYXiKeN18VEUSMwmWSy9dHgK7OeMg.PNG/image.png?type=w773)

* 18,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjdfMjI3/MDAxNzMyNzE1NjQxOTU5.z6TvVBugGmqvDi_GllVQ0H92BlvIM9dwzrlbr6wPsL4g.SPmZ7gcYAZLt5xb9HO-jQ8xTenAKMRnrvO2AjVLW0Rkg.PNG/image.png?type=w773)

```
user 수가 증가되면서 아래와 같은 오류가 나타난다.
```

```shell
error="Post \"http://localhost:8081/api/v1/user/5490/queue/token/issue\": dial tcp 127.0.0.1:8081: connectex: Only one usage of each socket address (protocol/network address/port) is normally permitted."
```

#### 원인

* 과도한 동시 요청

#### 결론

* **분당 최대 유저수는 12,000 명으로 분석**
* 이를 초과하면 시스템이 안정적으로 응답하지 못하고, 네트워크 자원이 포화 상태가 된다.



### **최적 처리 용량 테스트**

`공통 테스트: P90 < 1s / P95 < 1s`

* user 150

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMTEg/MDAxNzMyODI0ODI2NzQ1.febTMPEbCY8NT072_RqPHNU7mlw_6iSOmJZtvUoQ_PMg.GuIphZeU29c1PWCvhL3M7AENeVvI6Ji0nLih_xo51pEg.PNG/image.png?type=w773)

* user 350

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjY3/MDAxNzMyODI0ODUwNDU0.xkDRElcegZ3BaoLcgVlfriEhSrAP6bWM6kParZVk6WAg.HvZOOFcidXF4O9VRHouMISqTtDoAxaJE1SWDD6afJL4g.PNG/image.png?type=w773)

* user 400

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMTM3/MDAxNzMyODI0ODY2Mjgz.Q2nJLVcfi-ApRz4orzE1rxsa-7da-gXicTI1VtP6RJkg.Re5joK5QDTxolTCqd5FnrXIhR130vCFunptdtsNHA3Eg.PNG/image.png?type=w773)

* user 450

![img](https://postfiles.pstatic.net/MjAyNDExMjlfNzMg/MDAxNzMyODI0OTM5MDM4.-ByKj27GitUeR1pImaY3HmJZigIceIcXAlmbP-irTtgg.uHyq9W_bSc1PU_iryLB80q9B_EW3UTDTtBb5zTfMB88g.PNG/image.png?type=w773)

### 결론

* **분당 최적 유저수는 400명으로 분석**
* P90, P95 응답 시간이 모두 1초 이내로 유지되며 안정적인 성능을 나타낸다.



### 최대 VS 최적 시간 결과

| **항목**             | **최대 사용자 수** | **최소 사용자 수** |
| -------------------- | ------------------ | ------------------ |
| **부하 조건**        | 12,000 VU          | 400 VU             |
| **응답 시간 비교**   | -                  | -                  |
| - **평균 응답 시간** | 25.98s             | 0.44s              |
| - **p90 응답 시간**  | 47.84s             | 0.79s              |
| - **p95 응답 시간**  | 50.94s             | 0.97s              |

* 동시 요청 증가로 최대 사용자 수가 증가 시 평균 응답시간이 현저히 감소하였다. 
* **스케일링 전략**
  * **Auto Scaling **을 사용해 동시 요청 증가 시 인스턴스를 자동으로 추가하는 방안을 이용해 최대 요청시간을 줄일 수 있을 것으로 기대한다.



## 🧩 콘서트 목록 조회 테스트 결과

#### 테스트 스크립트

```javascript
import http from 'k6/http';
import { check, sleep, fail } from 'k6';

export let options = {
    startVUs: 0, // 시작 VU 수
    stages: [
    { duration: '15s', target: 500 },
    { duration: '15s', target: 900 },
    { duration: '15s', target: 500 },
    { duration: '15s', target: 0 },
    ],
    thresholds: {
        http_req_duration: ['p(90)<1000', 'p(95)<1000'], // 90% : 1초 미만/ 99% : 1초 미만
        http_req_failed: ['rate<0.01'],                  // 실패율 : 1% 미만
        checks: ['rate>0.99'],                           // 전체 요청 성공률 : 99% 이상
    },
    ext: {
      influxdb: {
        enabled: true,
        address: 'http://localhost:8086', 
        database: 'k6'
      },
    }
};

const BASE_URL = 'http://localhost:8081/api/v1';

const MAX_RETRIES = 5;

// 콘서트 목록 조회
function concertListRetry() {
    let retries = 0;
    let response;
    let success = false;

    while (retries < MAX_RETRIES && !success) {
        response = http.get(`${BASE_URL}/concerts/query`);
        success = check(response, {
            '콘서트 목록 조회 성공': (r) => r.status === 200,
        });

        if (!success) {
            retries++;
            sleep(1); 
        }
    }

    if (!success) {
        fail(`콘서트 목록 조회 실패: 재시도 횟수 ${MAX_RETRIES}, 상태 코드 ${response.status}, 응답: ${response.body}`);
    }
}

export default function () {  
    concertListRetry();
}
```



### **최대 처리 용량 테스트**

`주요 관점 포인트: 응답 성공`

* 10,000 user

#### 에러 발생

```shell
io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: Can't decode replay: +PONG

org.springframework.dao.QueryTimeoutException: Redis command timed out

Caused by: java.lang.OutOfMemoryError: Java heap space
```

#### 원인 분석

* **Redis 부하**:
  - 1만 건 이상의 콘서트 정보를 Redis에 캐싱하면서 **메모리 사용량이 급증**하고, 캐시 조회가 느려지는 문제가 발생.
  - Redis의 **메모리 제한 정책(Eviction Policy)** 미적용으로 메모리가 고갈되고, 캐시 접근 속도가 저하됨.
* **네트워크 병목**:
  - 대량의 Redis 데이터 응답으로 인해 **I/O 처리 지연**이 발생.
  - 네트워크 병목 현상(Network Bottleneck)으로 요청이 지연되면서 `Redis command timed out` 오류 발생.


#### 해결 방안

1. **[콘서트 목록 조회 시 10,000 건 조회.](https://github.com/crimsorry/hhplus-concert-service/blob/main/docs/%EC%BD%98%EC%84%9C%ED%8A%B8%20%EC%BF%BC%EB%A6%AC%20%EC%84%B1%EB%8A%A5%20%EA%B0%9C%EC%84%A0.md)** 
   * 해결 방안
     * 콘서트 목록 조회 시, **1페이지당 10건**의 데이터를 캐싱.
   * 사유
     * 페이지 단위로 데이터를 캐싱하여, 사용자 요청 시 한 번에 처리해야 하는 데이터 양을 최소화.
     * 사용자가 여러 페이지를 열람해도 Redis와 네트워크의 부담이 크게 줄어들며, 캐싱 데이터 크기를 효율적으로 관리 가능.
2. 리소스 부족
   * 해결 방안
     * JVM 힙 메모리 설정 `"-Xms512m -Xmx768m"`
     * memory 500m > 1g 변경
   * 사유
     * 캐싱 목록을 줄여도 리소스 에러가 나는 경우 정확한 원인 파악을 위해 heap memory 설정과 docker memory 설정을 변경해 각각의 경우의 수를 테스트 해보고자 한다.

### 테스트 실행

1. 캐싱 개선 : 10,000 건 > 1페이지 10건

[1페이지 10건]

* 6,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjhfMTQ2/MDAxNzMyNzMwNDAxMTg5.XGziHgQtx3-PzAybPYCg-AAajJfDa1AxEVTFHbA5aaMg.k0EaTLmDu_DV9DY6MKXq4XXCqWeJ0yVZGlldxRJH_3Yg.PNG/image.png?type=w773)

[1페이지 10건]

* 12,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjhfMTky/MDAxNzMyNzMwOTk2MTMz.5Wosow8U66SB5Ut0HUtgoSu4jAO9FhO7RfE059b3A5gg.hl1TWzG5_bLx7unsjpFDCKXjW5GlP0NnnqaW_YerDocg.PNG/image.png?type=w773)

#### 에러 발생

```shell
[isson-timer-6-1] o.r.c.handler.PingConnectionHandler      : Unable to send PING command over channel: [id: 0x2ca1ac18, L: - R:/:6379]
```

* 버전 이슈 문제 확인 후 redis image 버전 최신화

![img](https://postfiles.pstatic.net/MjAyNDExMjhfMjk5/MDAxNzMyNzMzMzk0NTEx.a2N-3i7K-_o25sQVRDgM9TQJwf4ZJQlpqmTDprCwySog.vUJXauP7UNwkJRg5LaEIn_itQK_e99PRh_Q_AM-dQG0g.PNG/image.png?type=w773)

#### **버전 업데이트 수행**

```
redis version 6.3.0 > redis:7.4.1
```

#### 업데이트 후 테스트 수행

`주요 관점 포인트: 응답 성공`

* 6,000 user

![image-20241128035644054](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20241128035644054.png)

#### 문제발생

* 응답시간 지연

| 케이스    | 이전     | 이후      |
| --------- | -------- | --------- |
| user      | 6,000 VU | 10,000 VU |
| 응답시간  | 6.16s    | 9.74s     |
| 캐시 미스 | 5건      | 24건      |

![img](https://postfiles.pstatic.net/MjAyNDExMjhfNCAg/MDAxNzMyNzM1MTc4MDM5.lmrLo6NZaQsHRGD-mkI50ufKY61xm60loNokw9DPLvAg.4bGRx3da597dzeYeDujTjlIFtA-b4AZWQzP07IfqWzMg.PNG/image.png?type=w773)



### 병목 지점 분석 1. 캐싱 전략 부재

콘서트 조회에 캐시가 제대로 적용되지 않을 수 있다는 생각이 들었다. 따라서 캐시 설정을 해제하고 테스트를 수행하였다.

![img](https://postfiles.pstatic.net/MjAyNDExMjhfMjEy/MDAxNzMyNzM2MTU4OTk5.WVytisje7bUqMqWLMEUiL2OjM--PNAJI1-sNTXFbcX0g.Z5lPWYFBCIpxOVw0hrlKP-2gfx5vFo9Wes1YI4Sim7Ag.PNG/image.png?type=w773)

* 결과적으로 기존에 비해 낮은 응답률과 낮은 성공 횟수를 보여주었다.
* 캐시 미적용 X



### 병목 지점 분석 2. 캐시 미스로 시간 증가

캐시 미스 횟수가 증가되면서 데이터베이스에서 데이터를 가져오게 되어 성능이 하락하였을 것으로 예상. 

**캐시 서버 메모리 용량을 늘려 성능 테스트 시도**

* cpu 1 core > 2 core 변경
* memory 500g > 2g 변경

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjQ0/MDAxNzMyODI0OTgxMjAw.UQN7KF6rcXE4rZCrhXY9Cbk497IIrsCKDLij7V9nzagg.fXGKDo0PQi46KRStNEf_WBVQ0Fb35-3sEDV4HqNzzGMg.PNG/image.png?type=w773)

* 결과적으로 user가 15,000 명일 경우 최소 9s ~ 최대 11s 사이 결과값이 나오는 것으로 확인. 
* 성능 이슈 X

### 병목 지점 분석 3. 유저 수 증가 문제

user 를 6,000 명, 10,000 명으로 증가 시켰기 때문에 캐시 미스가 늘어간 것으로 예상. 

* 6,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMTE3/MDAxNzMyODI0OTk1NDE0.6Uz0DAHLHM7_eKc_F6dd4i4xDhFJf9RnCjG3801nGp8g.hzYqN-Q4vPSjGfr2lOK4iU1241wfFNCtjLd9Vo894fwg.PNG/image.png?type=w773)

* 7,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMTU5/MDAxNzMyODI1MDA4OTE2.FZ6PZzonXgZHpKgvpZ0P8QhDZZ3vtMb5zTAQlTAdK-sg.WMlWqYpZq0jZymTeTahp6PU5xMM2Z4hfPmwlKT-MErog.PNG/image.png?type=w773)

* 8,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjQ5/MDAxNzMyODI1MDIzNzU0.VZB1M6YsQknBWtTJr2VjnAR8xJYWaAdZJsJfK_-W1EEg.F72g81amZRkbpR4JDTH7j4VF1AfOOEf9TGNUjd44dHAg.PNG/image.png?type=w773)

* 9,000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfNTIg/MDAxNzMyODI1MDQzNTA4.Sm5jafU9GyTOod49f5-7qk3nJzaZkcAZDDAcpktnUfwg.BAGzrrzreLYMU47xLX1geYnMjhuZXhHIOV8Ml2z5QHkg.PNG/image.png?type=w773)

#### 결과

| user  | 응답시간(s) | p90(s) | p95(s) |
| ----- | ----------- | ------ | ------ |
| 6,000 | 4.18        | 7.95   | 9.95   |
| 7,000 | 2.45        | 7.54   | 8.77   |
| 8,000 | 4.4         | 7.51   | 21.83  |
| 9,000 | 7.81        | 21.81  | 23.92  |

* user 수가 6,000 ~ 8,000 사이 일 때 응답시간이 일정하였으나 9,000 에 도달한 경우 응답시간과 p90 이 크게 늘어난 것으로 확인. 또한 p95 의 경우 user 8,000 부터 크게 증가 된 것으로 보인다. 

### 결론

* **분당 최대 유저수는 8,000명으로 분석**



### 기타 가설

1. **서킷 브레이커 도입**: 사용자 수가 증가함에 따라 캐시 미스(cache miss) 비율이 상승하고 있다. 특히 동시 접속자가 13,000명을 초과할 경우, 캐시 미스가 급격히 증가하여 응답 시간이 크게 늘어나고, 최악의 경우 응답 실패로 이어지게 된다. 이를 해결하기 위해 서킷 브레이커(Circuit Breaker) 패턴의 도입을 검토.
2. **Cluster Scaling**: 노드 추가로 데이터를 분산.
3. **High Associativity 구조 도입**: Redis 클러스터링이나 sharding 사용.



### **최적 처리 용량 테스트**

`공통 테스트: P90 < 1s / P95 < 1s`

* 400 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjcy/MDAxNzMyODI1MDU5MjUz.6aSwCrw5MEqx5OU-tIwncnVID7LMlNiLLNWDQGypxv8g.k40vjiD49UcD9G5ev9CjDPsvtIl-hkcBippEsna8o-Eg.PNG/image.png?type=w773)

* 500 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjA5/MDAxNzMyODI1MDczNjEx.F6eyGizrQTDe_8gnzxF55mlZYmkqKxvXtpjncrKfHL8g.stcFiCH4oOjtiUwL1uEHasZdrp2Mkflvf7rNpA58t7Ag.PNG/image.png?type=w773)

* 900 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMjk4/MDAxNzMyODI1MDk0NzM0.sRh50rNbRcQtl3mbKYyPI85rKQsWtO_Z_RssZpXj2x4g.W9s60gumByofWES1xRJaY7LBPT-esfP94VvqV0r0G54g.PNG/image.png?type=w773)

* 1000 user

![img](https://postfiles.pstatic.net/MjAyNDExMjlfMTcz/MDAxNzMyODI1MTEzMDAx.UMgFz7OGSeAwIUrIIsCj-MrOX8CEGC99HrreK6tBngEg.Ac8Mo-uBFnxZNMVw0pkZdbrR-2e5Nc-szq9B9xDOHqsg.PNG/image.png?type=w773)

### 결론

* **분당 최적 유저수는 900명으로 분석**
* P90, P95 응답 시간이 모두 1초 이내로 유지되며 안정적인 성능을 나타낸다.

### 최대 VS 최적 시간 결과

| **항목**             | **최대 사용자 수** | **최소 사용자 수** |
| -------------------- | ------------------ | ------------------ |
| **부하 조건**        | 8,000              | 900                |
| **응답 시간 비교**   |                    |                    |
| - **평균 응답 시간** | 4.4s               | 0.46s              |
| - **p90 응답 시간**  | 7.51s              | 0.81s              |
| - **p95 응답 시간**  | 21.83s             | 0.97s              |

* 대기성 생성에 비해 최대 사용자 수는 줄어들었지만, Redis 캐시를 사용해 평균 응답시간이 상당히 감소되었다.

* 최적 사용자 수는 **부하 조건이 약 2배 증가**(900명)했음에도 불구하고, 평균 응답 시간이 대기열 생성과 유사한 수준(0.46s)으로 나타남.

* Redis 캐시 활용 및 효율적인 부하 분산 덕분에 안정적인 성능 유지.

* **스케일링 전략**

  * **분산 캐시 클러스터링** : Redis Cluster를 구성하여 **수평 확장**(Horizontal Scaling) 설정. 노드 장애 시 고가용성(HA) 유지.



![image](https://private-user-images.githubusercontent.com/31988854/390899366-68c05a8a-3224-4991-abc1-985062f64384.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU0NzcsIm5iZiI6MTczMjgyNTE3NywicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTkzNjYtNjhjMDVhOGEtMzIyNC00OTkxLWFiYzEtOTg1MDYyZjY0Mzg0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMTkzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY5NThmZTY1YWIwYTQyMGY3N2U3YmU2MmI5ZTQxZjI1MDJhNjQxZDFiZTE0ZDUwODFkNzUyZjM5MjcxMDgwYmImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.cWPvD9ZtTAu5pZDn02JFObt56tzgQX7pgoGTm_J6Gt8)



## 🧩 대기열 생성 > 콘서트 일정 조회

#### 테스트 스크립트

```javascript
import http from 'k6/http';
import { check, sleep, fail } from 'k6';

export let options = {
    startVUs: 0, // 시작 VU 수
    stages: [
        { duration: '15s', target: 50 },
        { duration: '15s', target: 80 },
        { duration: '15s', target: 50 }
    ],
    thresholds: {
        http_req_duration: ['p(90)<2000', 'p(95)<2000'], // 90% : 2초 미만/ 99% : 2초 미만
        http_req_failed: ['rate<0.01'],                  // 실패율 : 1% 미만
        checks: ['rate>0.99'],                           // 전체 요청 성공률 : 99% 이상
    },
    ext: {
      influxdb: {
        enabled: true,
        address: 'http://localhost:8086', 
        database: 'k6'
      },
    }
};

const BASE_URL = 'http://localhost:8081/api/v1';
const userIdStart = 36;
const MAX_RETRIES = 5;

// 대기열 토큰 발급
function issueWaitingTokenWithRetry(userId) {
    let retries = 0;
    let response;
    let success = false;

    while (retries < MAX_RETRIES && !success) {
        response = http.post(`${BASE_URL}/user/${userId}/queue/token/issue`);
        success = check(response, {
            '토큰 발급 성공': (r) => r.status === 200,
        }); 

        if (!success) {
            retries++;
            sleep(1); 
        }
    }
    
    const responseBody = response && response.body ? response.json() : null;

    if (!success) {
        fail(`UserId: ${userId} 재시도 횟수: ${MAX_RETRIES}, 상태 코드 ${responseBodystatus}, 응답: ${responseBody.body}`);
    }

    return responseBody ? responseBody.waitingToken : null;
}

// 예약 가능 날짜 조회
function checkAvailableDatesWithRetry(concertId, waitingToken) {
    let retries = 0;
    let response;
    let success = false;

    while (retries < MAX_RETRIES && !success) {
        let headers = {
            'waitingToken': waitingToken, 
        };

        response = http.get(`${BASE_URL}/concert/${concertId}/date`, { headers });
        success = check(response, {
            '예약 가능 날짜 조회 성공': (r) => r.status === 200,
        });

        if (!success) {
            retries++;
            sleep(1); 
        }
    }

    if (!success) {
        const responseBody = response && response.body ? response.json() : null;
        fail(`ConcertId: ${concertId}, 재시도 횟수: ${MAX_RETRIES}, 상태 코드: ${response.status}, 응답: ${responseBody}`);
    }

    return response;
}


export default function () {
    let userId = userIdStart + __VU; 
    let concertId = 451;           
    let waitingToken = issueWaitingTokenWithRetry(userId);

    // Step 1: 대기열 토큰 발급
    if (!waitingToken) {
        fail('대기열 토큰 없음')
    }

    // Step 2: (active 스케줄러 대기)
    sleep(40);

    // Step 3: 예약 가능 날짜 조회
    for (let i = 0; i < 5; i++) {
        checkAvailableDatesWithRetry(concertId, waitingToken);
        sleep(1); // 호출 간격
    }
}
```



### **최대 처리 용량 테스트**

`주요 관점 포인트: 응답 성공`

* 600 user

```
10초당 100명씩 스케줄링 시 > 분당 최대 600명

대기열 생성에서 밝혀진 최대 동시 접속자 수는 15,000, 콘서트 조회에서는 12,000 명이다 (그 이상도 가능하지만 성능 저하 문제)

콘서트에는 다수의 사람이 몰리기 때문에 10초당 대기열 진입 사용자 수를 100명으로 한정하여 테스트 진행
```

#### 에러 발생

```shell
Unable to acquire JDBC Connection [HikariPool-1 - Connection is not available, request timed out after 30000ms (total=10, active=10, idle=0, waiting=27)] 
```

* 확인 결과 connection poll 오류로 생긴 문제 

| 이름    | 현황 | 뜻                  |
| ------- | ---- | ------------------- |
| total   | 10   | 전체 연결의 개수    |
| active  | 10   | 사용중인 연결 개수  |
| idle    | 0    | 놀고 있는 연결 개수 |
| waiting | 27   | 기다리는 요청 개수  |

#### 해결

* `maximum-pool-size : 25` 설정
  * waiting 요청 개수에 맞게 연결 갯수 수정

![image](https://private-user-images.githubusercontent.com/31988854/390899419-88dd00b9-a6bc-4742-ad38-a9b33e899c28.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU1MDksIm5iZiI6MTczMjgyNTIwOSwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk0MTktODhkZDAwYjktYTZiYy00NzQyLWFkMzgtYTliMzNlODk5YzI4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjAwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEyMWI4ODY3ZjVjZGFjNzNkMGJiNzI1MmQxZGI3N2Y3YjM5OGU0MTM1YTRlNGFiOTcwMTM0MTI5NTI3MDJiNzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.uY0r1tjBTTlUNkku9Um-C_mVCtiKFWEbLGLWjBUvHjU)

* 95% 요청이 성공하게 되면서 `connection poll waiting : 57 건`으로 표출되었다. 

#### 병목 지점 분석

1. **application 부하 조절**
   * 대기열 진입 수를 **10초당 50명**으로 제한하는 방식으로 부하 테스트를 진행한다.
2. **HikariCP 풀 크기 조정**
   * 동시 접속 풀이 고갈되는 상황을 방지하기 위해 `maximum-pool-size`를 증가시킨다.
   * 하지만 풀 크기를 단순히 늘리는 것은 단기적인 해결책일 뿐, 장기적으로 부하가 반복적으로 발생하는 경우 근본적인 해결책이 될 수 없다.
     * **WHY?:** 커넥션 풀이 크기를 늘리면 데이터베이스와의 연결 비용이 증가하여 시스템 자원(메모리, CPU) 소모가 커질 가능성이 있다. 
3. **서킷 브레이커 도입**
   * 서킷 브레이커를 활용하여 임계치 도달 시 **슬라이딩 윈도우(sliding window)**를 통해 장애 상황을 감지하고, 특정 시간 동안 일부 요청을 차단한다. 
   * 이를 통해 장애가 발생한 부분에 대해 빠른 대체 로직(`Fallback`)을 실행하여 시스템 전체의 가용성을 유지한다.
   * 일정 시간 동안 대기열 API 요청을 차단하고, 사용자에게 "잠시 후 다시 시도"라는 메시지를 반환.

->  풀 크기를 늘리는 방안은 단기적으로 문제를 해결할 수 있으나, 부하가 반복적으로 발생할 때마다 풀 크기를 조정하는 것은 적절하지 않다.  따라서 **application 부하를 조절**하는 방향으로 테스트를 진행하고자 한다.

#### 테스트

* 300 user


![image](https://private-user-images.githubusercontent.com/31988854/390899459-2f650c16-7879-49ab-8243-2f36787a121c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU1MjUsIm5iZiI6MTczMjgyNTIyNSwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk0NTktMmY2NTBjMTYtNzg3OS00OWFiLTgyNDMtMmYzNjc4N2ExMjFjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjAyNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQwM2QzMTA2MTU4Y2UyZmYzZTQ3MmQ5M2FiODA4MThjNzgxYTI1MDhmN2MzMDhkM2M4ZmFmOTZlOGJjYjExYTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.c6uG66uiTrefzjIhDCMjtQa3WtlWuuC5dHIlnR_pCYA)

### 결론

* **분당 최대 유저수는 300명으로 분석**
* **application 부하**를 조절하여 최대 유저 수를 테스트하였다.



### **최적 처리 용량 테스트**

`공통 테스트: P90 < 2.5s / P95 < 2.5s`

* 150 user

![image](https://private-user-images.githubusercontent.com/31988854/390899494-9a8ba79f-56c3-425b-8f05-e883989f3891.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU1NTIsIm5iZiI6MTczMjgyNTI1MiwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk0OTQtOWE4YmE3OWYtNTZjMy00MjViLThmMDUtZTg4Mzk4OWYzODkxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjA1MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhODRkMGE4OWQyYWE4Y2UxMjFlZmVmZjcwNTM5ZDk0ZTU2ZDdlYjFjMzA1MzI4MTdjODdiYTI5OGMzYWVhYTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.spwC3PmOEejS3AxMQGRDRdgc_Ij-_IPNMAoB1zIhhE4)

* 100 user

![image](https://private-user-images.githubusercontent.com/31988854/390899505-8e537047-c0bf-414a-8603-507f0f940700.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU1NTIsIm5iZiI6MTczMjgyNTI1MiwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk1MDUtOGU1MzcwNDctYzBiZi00MTRhLTg2MDMtNTA3ZjBmOTQwNzAwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjA1MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWIwYjE1YTU1NWY1OTYyNGY4YmRkOTNhNWIyNzVhZTljNmZlNTQ1MjA2ZGM2MDgzMTI3NDI5Mzg5MDE0NTdmNjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.jsz4o8Ur2_SzL3zDHWwJX0WaGQIPPhh3bEGM9JPsKY4)

* 70 user

![image](https://private-user-images.githubusercontent.com/31988854/390899515-1a7002c4-1da4-4703-b64e-50d2a1a56cbd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU1NTIsIm5iZiI6MTczMjgyNTI1MiwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk1MTUtMWE3MDAyYzQtMWRhNC00NzAzLWI2NGUtNTBkMmExYTU2Y2JkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjA1MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI3MjU1OTY2MWIxZTRhMzg3OWJiZjllMTUzMGRmYzFiYzk1NTgyNGQyNzQwYjgyOWRiZWYzOTU1ZThmY2QyOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.0OCXifpvGRpiG5hv1RDKE7OXMfYjmVMu3xi1OBWHkVA)

### 결론

* **분당 최적 유저수는 70명으로 분석**

### 최대 VS 최적 시간 결과

| **항목**             | **최대 사용자 수** | **최소 사용자 수** |
| -------------------- | ------------------ | ------------------ |
| **부하 조건**        | 300                | 70                 |
| **응답 시간 비교**   |                    |                    |
| - **평균 응답 시간** | 9.71s              | 0.23s              |
| - **p90 응답 시간**  | 32.08s             | 1.07               |
| - **p95 응답 시간**  | 38.02s             | 1.61               |

* 동시 요청 증가로 최대 사용자 수가 증가 시 평균 응답시간이 현저히 감소하였다. 
* **최적화 전략**
  * **데이터베이스 부하 증가**
    * 모든 요청이 데이터베이스로 직접 전달되면서 트래픽이 집중되고, 과부하가 발생.
  * **캐시 부재**
    * 자주 호출되는 데이터(날짜 조회)가 캐싱되지 않아 데이터베이스에 불필요한 반복 요청 발생.
    * **Redis TTL** 설정 및 **Eviction Policy** 설정으로 자주 사용되지 않는 데이터를 제거하여 성능 최적화
  * **데이터 베이스 최적화**
    * 쿼리에 적절한 **인덱스 최적화**가 부족하여 데이터 조회 시 불필요한 테이블 스캔이 발생.
    * 인덱스 추가 및 쿼리 구조 개선을 통해 데이터 조회 속도 향상이 필요



![image](https://private-user-images.githubusercontent.com/31988854/390899914-57b93fb9-3b28-4a1a-951d-d71e7743cf8a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI4MjU3MzEsIm5iZiI6MTczMjgyNTQzMSwicGF0aCI6Ii8zMTk4ODg1NC8zOTA4OTk5MTQtNTdiOTNmYjktM2IyOC00YTFhLTk1MWQtZDcxZTc3NDNjZjhhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDIwMjM1MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWM3MDgzYjQxZGE3YWU5M2Q1YzhjYTRhYzZjOWY4YzZjYjc3M2RkM2JlZjhjNzc4Njg2NzNiMjYzMjY4MjQ3NzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.foUrqwcL9afWcKw89EPgrWEHFsvt6_a5BghlukDUUys)



## 🧩 장애 대응 문서 (가상)

#### 1. 장애 사례 및 대응 방안

**1-1 동시 요청 폭증 대응 전략**

* **문제점**: 대규모 동시 요청으로 인한 서버 리소스 고갈
* **해결 방안**: Circuit Breaker 패턴 구현 (Netflix Hystrix, Resilience4j)

**1-2 Redis 경합 문제 발생**

* **문제점**: 동시 트랜잭션으로 인한 Redis 커넥션 포화
* **해결 방안**: Redis 분산락 구현 or 비동기처리

**1-3 데이터베이스 부하**

* **문제점**: 대량 동시 쿼리로 인한 데이터베이스 성능 저하
* **해결 방안**: L2 캐시 도입 or 인덱스 최정화 및 쿼리 튜닝

#### 2. 장애 발생 대응 전략

**2-1 지속적 모니터링**

* **시스템 성능 모니터링**
  - **도구**: Grafana, Prometheus.
  - 서버 CPU, 메모리, 네트워크 트래픽, 데이터베이스 연결 상태 실시간 모니터링.
* **로그 기반 알림 시스템 구축**
* **자동화된 헬스 체크**
* **장애 로그 수집**
